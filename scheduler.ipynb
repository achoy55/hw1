{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "from tzlocal import get_localzone\n",
    "from datetime import datetime\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import train_model as tm\n",
    "import data_processing as dp\n",
    "from data_loader import load_data_at_start_date, load_data_period\n",
    "from pub_sub import Publisher, Subscriber\n",
    "from features import FeatureEngineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_params = {\n",
    "    'emaf': 20,\n",
    "    'emam': 100,\n",
    "    'emas': 150,\n",
    "    'rsi': 14,\n",
    "    'macd': [12, 26, 9],\n",
    " }\n",
    "\n",
    "fe = FeatureEngineering(fe_params)\n",
    "publisher = Publisher()\n",
    "subscriber = Subscriber(\"Subscriber\")\n",
    "publisher.subscribe(subscriber)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(params):   \n",
    "    data_store = params['data_store_dir']\n",
    "    file_compress = params['file_compress']\n",
    "    tickers = params['tickers']\n",
    "    trend_features = params['trend_features']\n",
    "    \n",
    "    for ticker in tqdm(tickers):\n",
    "        data_with_trend = dp.get_data(data_store, ticker, compress=file_compress) ## Data contains features and TA indicators\n",
    "        print(f'Train data length: {len(data_with_trend)}')\n",
    "        # print(data_with_trend.isnull().sum())\n",
    "        # print(data_with_trend.tail(5))\n",
    "\n",
    "        features = [d[ticker] for d in trend_features if ticker in d.keys()][0] + params['trend_indicators']\n",
    "        data_with_features = data_with_trend[features + params['target_columns']]\n",
    "\n",
    "        params['features'] = features\n",
    "        tm.predict_process(data_with_features, params)\n",
    "\n",
    "    print(f'=== Finish Train models ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trend_data(data, params):\n",
    "    features = params['OHLCV']\n",
    "    lag_periods = params['lag_periods']\n",
    "    min_outliers=params['min_outliers']\n",
    "    max_outliers=params['max_outliers']\n",
    "\n",
    "    df = fe.clear_invalid_targets(fe.add_target(fe.enrich_with_indicators(data), lag_periods))\n",
    "    # df = fe.clear_invalid_targets(fe.add_target2(fe.enrich_with_indicators(data)))\n",
    "    df = fe.validate_outliers(df, 'Close', min_outliers, max_outliers)\n",
    "    # # print(df.isnull().sum())\n",
    "    \n",
    "    return fe.create_trend_features(df, features, lag_periods) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation(params):\n",
    "    print('=== Start feature generation ===')\n",
    "    \n",
    "    tickers = params['tickers']\n",
    "    data_store = params['data_store_dir']\n",
    "    new_data_dir = params['new_data_dir']\n",
    "    file_compress = params['file_compress']\n",
    "\n",
    "    trend_features = list()\n",
    "    for ticker in tqdm(tickers):\n",
    "        print(f'=== Feature for ticker: {ticker}')\n",
    "\n",
    "        new_df = dp.get_data(new_data_dir, ticker, compress=False) # get new raw data\n",
    "        new_df.dropna(inplace=True)\n",
    "\n",
    "        data_with_trend, new_trend_features = get_trend_data(new_df, params)\n",
    "        # print(data_with_trend.isnull().sum())\n",
    "        # print(data_with_trend.tail(5))\n",
    "        dp.merge_and_store_new_data(data_with_trend, ticker, data_store, compress=file_compress) # merge with old and check duplication\n",
    "        \n",
    "        trend_features.append({\n",
    "            ticker: new_trend_features\n",
    "        })\n",
    "\n",
    "        ## Plot\n",
    "        if params['is_plot']:\n",
    "            dp.plot_data(data_with_trend)\n",
    "            \n",
    "    print('=== Finish feature engineering ===')\n",
    "    return trend_features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    models = list()\n",
    "    # models.append(tm.ModelFunc.LOGISTIC_REG)\n",
    "\n",
    "    # models.append(tm.ModelFunc.LINEAR_REG)\n",
    "    # models.append(tm.ModelFunc.KNN_REG) \n",
    "    # models.append(tm.ModelFunc.DECISION_TREE_REG)\n",
    "    # models.append(tm.ModelFunc.RANDOM_FOREST_REG)\n",
    "    # models.append(tm.ModelFunc.CATBOOST_REG)\n",
    "    # models.append(tm.ModelFunc.XGBOOST_REG)\n",
    "\n",
    "    # models.append(tm.ModelFunc.XGBOOST_CLASS)\n",
    "    # models.append(tm.ModelFunc.CATBOOST_CLASS)\n",
    "\n",
    "    # models.append(tm.ModelFunc.RANDOM_FOREST_CLASS)\n",
    "    models.append(tm.ModelFunc.DECISION_TREE_CLASS)\n",
    "    models.append(tm.ModelFunc.KNN_CLASS)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing job scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'new_data_dir': 'crypto_data',\n",
    "    'data_store_dir': '_data_store',\n",
    "    'time_interval': '1d',\n",
    "    'period': -(datetime.now() - datetime(2019, 1, 1)).days,\n",
    "    'tickers': ['BTC-USD'],\n",
    "    # 'tickers': ['BTC-USD', 'ETH-USD', 'SOL-USD', 'XRP-USD'],\n",
    "\n",
    "    'lag_periods': 3, #7\n",
    "    'min_outliers': .23,\n",
    "    'max_outliers': .77,\n",
    "    'trend_indicators': [ 'emaf', 'emam', 'emas', 'rsi', 'macd', 'adx'],\n",
    "    'target_columns': ['Target'],\n",
    "    'OHLCV': ['Open', 'High', 'Low', 'Close', 'Volume'],\n",
    "\n",
    "    # 'max_train_size': 120,\n",
    "    # 'test_size': 60,\n",
    "    # 'max_train_size': 90,\n",
    "    # 'test_size': 30,\n",
    "    'max_train_size': 180,\n",
    "    'test_size': 90,\n",
    "\n",
    "    'file_compress': True,\n",
    "    'use_stacking': True,\n",
    "    'use_blending': False,\n",
    "    'model_funcs': get_models(),\n",
    "    'train_func': train_model,\n",
    "    'is_train': True,\n",
    "    'is_plot': False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_job(params):\n",
    "    new_data_dir = params['new_data_dir']\n",
    "    tickers = params['tickers']\n",
    "    period = params['period']\n",
    "    time_interval = params['time_interval']\n",
    "\n",
    "    print(f'=== Start job: {datetime.now()} ===')\n",
    "    \n",
    "    # Load new data\n",
    "    crypto_dir = load_data_at_start_date(tickers, period, time_interval, new_data_dir)\n",
    "    # crypto_dir = load_data_period(tickers, datetime(2019, 1, 1), datetime(2024, 12, 31), time_interval)\n",
    "\n",
    "    trend_features = feature_generation(params)\n",
    "\n",
    "    if params['is_train']:\n",
    "        params['trend_features'] = trend_features\n",
    "        train_model(params)\n",
    "    \n",
    "    print(f'=== Finish job: {datetime.now()} ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Start job: 2025-01-08 22:10:41.238909 ===\n",
      "Start load data, tickers ['BTC-USD'], interval: 1d, from: -2199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2200 entries, 2019-01-01 to 2025-01-08\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   (BTC-USD, Open)       2200 non-null   float64\n",
      " 1   (BTC-USD, High)       2200 non-null   float64\n",
      " 2   (BTC-USD, Low)        2200 non-null   float64\n",
      " 3   (BTC-USD, Close)      2200 non-null   float64\n",
      " 4   (BTC-USD, Adj Close)  2200 non-null   float64\n",
      " 5   (BTC-USD, Volume)     2200 non-null   int64  \n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 120.3 KB\n",
      "Download data completed\n",
      "=== Start feature generation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature for ticker: BTC-USD\n",
      "Outliers detected: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Finish feature engineering ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length for train: 2045\n",
      "=== Start Train models:\n",
      " [<function decision_tree_classifier_model at 0x7fb3b2457c40>, <function knn_classifier_model at 0x7fb3b2457d80>] ===\n",
      "     Train size: 360, Val size: 3240, Test size: 1800\n",
      "Pred Train size: 360, Val size: 3240, Test size: 1800\n",
      "=== Train sample metrics ===\n",
      "ROC AUC: 1.0000\n",
      "   Cutoff  Precision  Recall  Accuracy  F1-Score\n",
      "0    50.0      100.0   100.0     100.0     100.0\n",
      "1    60.0      100.0   100.0     100.0     100.0\n",
      "2    70.0      100.0   100.0     100.0     100.0\n",
      "3    80.0      100.0   100.0     100.0     100.0\n",
      "=== Val sample metrics ===\n",
      "ROC AUC: 0.5108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Cutoff  Precision     Recall   Accuracy   F1-Score\n",
      "0    50.0  53.212521  57.235676  51.358025  55.150825\n",
      "1    60.0  53.212521  57.235676  51.358025  55.150825\n",
      "2    70.0  53.212521  57.235676  51.358025  55.150825\n",
      "3    80.0  53.212521  57.235676  51.358025  55.150825\n",
      "=== Test sample metrics ===\n",
      "ROC AUC: 0.4981\n",
      "   Cutoff  Precision     Recall   Accuracy   F1-Score\n",
      "0    50.0  53.745541  46.549949  49.555556  49.889625\n",
      "1    60.0  53.745541  46.549949  49.555556  49.889625\n",
      "2    70.0  53.745541  46.549949  49.555556  49.889625\n",
      "3    80.0  53.745541  46.549949  49.555556  49.889625\n",
      "===========================\n",
      "=== Finish Train models ===\n",
      "=== Finish job: 2025-01-08 22:10:43.178137 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_job(params)\n",
    "\n",
    "# scheduler = BlockingScheduler(job_defaults={'misfire_grace_time': 15*60})\n",
    "# scheduler = BackgroundScheduler(job_defaults={'misfire_grace_time': 15*60})\n",
    "# # scheduler.add_job(start_job, 'cron', day_of_week='mon-fri', hour='*/4', minute=5, jitter=120, timezone=get_localzone())\n",
    "# scheduler.add_job(start_job, 'interval', seconds=15, args=[params])\n",
    "# scheduler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler.shutdown(wait=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
