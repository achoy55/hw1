{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "# служебные функции\n",
    "from adtk.data import validate_series\n",
    "from adtk.visualization import plot\n",
    "# Статистические методы детектирования точечных аномалий\n",
    "from adtk.detector import ThresholdAD\n",
    "from adtk.detector import QuantileAD\n",
    "from adtk.detector import InterQuartileRangeAD\n",
    "from adtk.detector import GeneralizedESDTestAD\n",
    "# Статистические методы детектирования групповых аномалий\n",
    "from adtk.detector import PersistAD\n",
    "from adtk.detector import LevelShiftAD\n",
    "from adtk.detector import VolatilityShiftAD\n",
    "# методы на основе декомпозиции временного ряда и авторегрессии\n",
    "from adtk.detector import SeasonalAD\n",
    "from adtk.detector import AutoregressionAD\n",
    "# Методы на основе кластеризации - неконтролируемое обучение\n",
    "from adtk.detector import MinClusterDetector\n",
    "from sklearn.cluster import KMeans\n",
    "# Методы на основе плотности\n",
    "from adtk.detector import OutlierDetector\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "# Методы на основе регрессии - контролируемое обучение\n",
    "from adtk.detector import RegressionAD\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Методы на основе понижения размерности\n",
    "from adtk.detector import PcaAD\n",
    "# кастомизация\n",
    "from adtk.detector import CustomizedDetectorHD\n",
    "from adtk.transformer import ClassicSeasonalDecomposition\n",
    "from adtk.pipe import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ThresholdAD(df, high, low):\n",
    "    s = validate_series(df)\n",
    "    threshold_ad = ThresholdAD(high=high, low=low)\n",
    "    return threshold_ad.detect(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_QuantileAD(df, high, low):\n",
    "    s = validate_series(df)\n",
    "    quantile_ad = QuantileAD(high=high, low=low)\n",
    "    return quantile_ad.fit_detect(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_InterQuartileRangeAD(df, c):\n",
    "    s = validate_series(df)\n",
    "    iqr_ad = InterQuartileRangeAD(c=c)\n",
    "    return iqr_ad.fit_detect(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_SeasonalAD(df, c=3.0, side=\"both\"):\n",
    "    s = validate_series(df)\n",
    "    seasonal_ad = SeasonalAD(c=c, side=side)\n",
    "    return seasonal_ad.fit_detect(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_AutoregressionAD(df, n_steps=7*2, step_size=24, c=3.0):\n",
    "    s = validate_series(df)\n",
    "    autoregression_ad = AutoregressionAD(n_steps=n_steps, step_size=step_size, c=c)\n",
    "    return autoregression_ad.fit_detect(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_MinClusterDetector(df, n_clusters=3):\n",
    "    s = validate_series(df)\n",
    "    min_cluster_detector = MinClusterDetector(KMeans(n_clusters=n_clusters))\n",
    "    return min_cluster_detector.fit_detect(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_OutlierDetector(df, contamination=0.05):\n",
    "    s = validate_series(df)\n",
    "    outlier_detector = OutlierDetector(LocalOutlierFactor(contamination=contamination))\n",
    "    return outlier_detector.fit_detect(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_RegressionAD(df, target='Close', c=3.0):\n",
    "    s = validate_series(df)\n",
    "    regression_ad = RegressionAD(regressor=LinearRegression(), target=target, c=c)\n",
    "    return regression_ad.fit_detect(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_z(df, score, column_name):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Z-score'] = stats.zscore(df_copy[column_name])\n",
    "    anomalies = df_copy[abs(df_copy['Z-score']) > score]\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_anomalies_z(df, column_name, score):\n",
    "    detect_anomalies = detect_anomalies_z(df, column_name, score)\n",
    "    anomalies = pd.concat(anomalies, detect_anomalies)\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_with_zcore(df, window, score):\n",
    "    clean_function = lambda x: x[np.abs(stats.zscore(x)) < score]\n",
    "    return df.rolling(window=window).apply(clean_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_rating_z_between_column(df, close_column, volume_column):\n",
    "    df_copy = df.copy()\n",
    "    anomalies_df_close = stats.zscore(df_copy[close_column])\n",
    "    anomalies_df_volume = stats.zscore(df_copy[volume_column])\n",
    "    # оценка риска\n",
    "    close_risk = anomalies_df_close['Z-score'].apply(lambda x: abs(x).mean())\n",
    "    volume_risk = anomalies_df_volume['Z-score'].apply(lambda x: abs(x).mean())\n",
    "\n",
    "    total_risk = close_risk + volume_risk\n",
    "\n",
    "    # нормализация\n",
    "    return (total_risk - total_risk.min()) / (total_risk.max() - total_risk.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies_by_column(data, anomalies, y_column):\n",
    "    anomaly_df = anomalies[anomalies==True]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=data.index, y=data[y_column], mode='lines', name='Value'))\n",
    "    fig.add_trace(go.Scatter(x=anomaly_df.index, y=data[y_column], mode='markers', name='Anomaly',\n",
    "                             marker=dict(color='red')))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies(data, anomalies):\n",
    "    anomaly_df = anomalies[anomalies==True]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=data.index, y=data, name='value', line=dict(color='green', width=2)))\n",
    "    fig.add_trace(go.Scatter(x=anomaly_df.index, y=data, name='anomaly', mode='markers'))\n",
    "    fig.update_layout(title='Anomalies', xaxis_title='Date', yaxis_title='Value')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies_custom(df, y_column, config):\n",
    "    figure = go.Figure()\n",
    "    # plot baseline     \n",
    "    figure.add_trace(go.Scatter(name=y_column, x = df.index, y=df[y_column], marker=dict(color='green')))\n",
    "         \n",
    "    # plot anomaly points     \n",
    "    anomaly_df = df\n",
    "    anomaly_df = anomaly_df[anomaly_df[config['anomaly_column']]==True]\n",
    "            \n",
    "    figure.add_trace(go.Scatter(name=config['legend_name'], x = anomaly_df.index, y=anomaly_df[y_column], \n",
    "        mode='markers',\n",
    "        marker=dict(color=config['color'],size=10)))\n",
    "\n",
    "    figure.update_layout(title= 'Anomalies', xaxis_title='date', yaxis_title='value', legend_title=\"Anomaly Type\",)\n",
    "    \n",
    "    figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trend_features(df, features, lag_periods):\n",
    "    \"\"\"\n",
    "    Добавляет классические финансовые признаки: отношение к предыдущим периодам, логарифмические изменения и индикаторы трендов.\n",
    "    \n",
    "    df: DataFrame с исходными данными\n",
    "    features: список признаков, для которых необходимо добавить индикаторы\n",
    "    lag_periods: сколько периодов назад учитывать для расчетов\n",
    "    \n",
    "    Возвращает:\n",
    "    - обновленный DataFrame с новыми фичами\n",
    "    - список новых колонок, которые можно использовать как признаки\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Работаем с копией DataFrame\n",
    "    new_columns = []  # Список для хранения новых колонок\n",
    "    \n",
    "    for feature in features:\n",
    "        # Отношение текущего значения к предыдущему (лаг = 1)\n",
    "        df[f'{feature}_ratio_1'] = df[feature] / df[feature].shift(1)\n",
    "        new_columns.append(f'{feature}_ratio_1')\n",
    "        \n",
    "        # Логарифмическое изменение (логарифм отношения текущего значения к предыдущему)\n",
    "        df[f'{feature}_log_diff_1'] = np.log(df[feature] / df[feature].shift(1))\n",
    "        new_columns.append(f'{feature}_log_diff_1')\n",
    "        \n",
    "        # Momentum (разница между текущим значением и значением N периодов назад)\n",
    "        df[f'{feature}_momentum_{lag_periods}'] = df[feature] - df[feature].shift(lag_periods)\n",
    "        new_columns.append(f'{feature}_momentum_{lag_periods}')\n",
    "        \n",
    "        # Rate of Change (ROC): процентное изменение за N периодов\n",
    "        df[f'{feature}_roc_{lag_periods}'] = (df[feature] - df[feature].shift(lag_periods)) / df[feature].shift(lag_periods) * 100\n",
    "        new_columns.append(f'{feature}_roc_{lag_periods}')\n",
    "        \n",
    "        # Exponential Moving Average (EMA) с периодом N\n",
    "        df[f'{feature}_ema_{lag_periods}'] = df[feature].ewm(span=lag_periods, adjust=False).mean()\n",
    "        new_columns.append(f'{feature}_ema_{lag_periods}')\n",
    "    \n",
    "    # Удаление строк с NaN значениями, которые появились из-за сдвигов\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df, new_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import import_ipynb\n",
    "# from file_loader import loading_v3\n",
    "\n",
    "# dir = \"crypto_data\"\n",
    "# symbol = 'BTC-USD'\n",
    "# df = pd.DataFrame()\n",
    "# try:\n",
    "#     f = os.path.join(dir, symbol+'.csv')\n",
    "#     if os.path.isfile(f):\n",
    "#         df = loading_v3(f)\n",
    "#     df = df.dropna()\n",
    "#     df.set_index('Date', inplace=True)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading file {symbol}.csv: {e}\")\n",
    "\n",
    "# close_df = df['Close'].copy()\n",
    "# close_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomalies = detect_QuantileAD(df['Volume'], 0.99, 0.01)\n",
    "# anomalies = detect_InterQuartileRangeAD(df['Volume'], 1.5)\n",
    "# anomalies = detect_ThresholdAD(df['Volume'], 30, 15)\n",
    "# anomalies = detect_InterQuartileRangeAD(df['Volume'], 1.5)\n",
    "# anomalies = detect_AutoregressionAD(close_df, 7*2, 6, 3.0)\n",
    "# anomalies = detect_SeasonalAD(df['Volume'], 3.0, 'both')\n",
    "\n",
    "# # plot(df['Volume'], anomaly=anomalies, ts_markersize=1, anomaly_color='red', anomaly_tag=\"marker\", anomaly_markersize=2);\n",
    "\n",
    "# # anomalies.value_counts()\n",
    "# # anomalies.window = 30\n",
    "\n",
    "# plot_anomalies_by_column(df, anomalies, 'Volume')\n",
    "# plot_anomalies(df['Volume'], anomalies)\n",
    "\n",
    "\n",
    "\n",
    "# # anomalies = detect_MinClusterDetector(df, 3) # must be pandas Dataframe\n",
    "# # anomalies = detect_OutlierDetector(df, 0.05) # ERROR\n",
    "# # anomalies = detect_RegressionAD(df, 'Volume', 3.0) # ERROR\n",
    "# # plot(df, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_color='red', anomaly_alpha=0.3, curve_group='all');\n",
    "\n",
    "# # print(anomalies.value_counts())\n",
    "# # plot(close_df, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_markersize=5, ts_color='g', anomaly_color='red', anomaly_tag=\"marker\");\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
