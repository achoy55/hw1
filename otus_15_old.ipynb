{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import hstack\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import import_ipynb\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import talib as ta\n",
    "\n",
    "from redis_cli import RedisClient\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from file_loader import get_data, store_to_file\n",
    "from features import FeatureEngineering\n",
    "from data_loader import load_crypto_data, load_crypto_data2\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = RedisClient(db=1, username='usr_redis', password='usr_pwd')\n",
    "# r.test_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "period=-(datetime.now() - datetime(2019, 1, 1)).days\n",
    "# period=-(datetime(2024,12,27) - datetime(2019, 1, 1)).days\n",
    "time_interval='1d'\n",
    "tickers = ['BTC-USD', 'ETH-USD'] #, 'SOL-USD', 'XRP-USD'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load crypto data, tickers ['BTC-USD', 'ETH-USD'], interval: 1d, from: 2019-01-01 16:08:51.437605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2198 entries, 2019-01-01 to 2025-01-06\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   (ETH-USD, Open)       2198 non-null   float64\n",
      " 1   (ETH-USD, High)       2198 non-null   float64\n",
      " 2   (ETH-USD, Low)        2198 non-null   float64\n",
      " 3   (ETH-USD, Close)      2198 non-null   float64\n",
      " 4   (ETH-USD, Adj Close)  2198 non-null   float64\n",
      " 5   (ETH-USD, Volume)     2198 non-null   int64  \n",
      " 6   (BTC-USD, Open)       2198 non-null   float64\n",
      " 7   (BTC-USD, High)       2198 non-null   float64\n",
      " 8   (BTC-USD, Low)        2198 non-null   float64\n",
      " 9   (BTC-USD, Close)      2198 non-null   float64\n",
      " 10  (BTC-USD, Adj Close)  2198 non-null   float64\n",
      " 11  (BTC-USD, Volume)     2198 non-null   int64  \n",
      "dtypes: float64(10), int64(2)\n",
      "memory usage: 223.2 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %run crypto_data_loader.ipynb\n",
    "crypto_dir = load_crypto_data(tickers, period, time_interval)\n",
    "# crypto_dir = load_crypto_data2(tickers, datetime(2019, 1, 1), datetime(2024, 12, 31), time_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_and_plot(data, column_name1, column_name2):\n",
    "    df = data.copy()\n",
    "    X_train = df[column_name1]\n",
    "    y_train = df[column_name2]\n",
    "\n",
    "    # without normalization\n",
    "    model_no_norm = ut.linear_regression(X_train, y_train)\n",
    "    weights_no_norm = model_no_norm.coef_\n",
    "\n",
    "    # normalize data\n",
    "    prices_norm, volume_norm = ut.normalize_MinMax_by_column(X_train, y_train)\n",
    "    # print(prices_norm)\n",
    "\n",
    "    model_with_norm = ut.linear_regression(prices_norm, volume_norm)\n",
    "    weights_with_norm = model_with_norm.coef_\n",
    "\n",
    "    # Plotting the weights\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].bar(['Price', 'Volume'], weights_no_norm, color='red')\n",
    "    axes[0].set_title('Weigth without normalization')\n",
    "    axes[0].set_ylabel('Model weghts')\n",
    "\n",
    "    axes[1].bar(['Price', 'Volume'], weights_with_norm, color='green')\n",
    "    axes[1].set_title('Weigth after normalization')\n",
    "    axes[1].set_ylabel('Model weghts')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_type = file, redis\n",
    "def merge_and_store_data(new_df, key, store_type='file', compress=False):\n",
    "    if store_type == 'redis':\n",
    "        saved_data = r.get_key(key)\n",
    "    else:\n",
    "        saved_data = get_data('_data_store', key, compress=compress)\n",
    "\n",
    "    merged_df = ut.validate_duplicate_and_merge(saved_data, new_df)\n",
    "\n",
    "    if store_type == 'redis':\n",
    "        r.set_key(key, merged_df)\n",
    "    else:\n",
    "        store_to_file(merged_df, key, compress=compress)\n",
    "\n",
    "    merged_df.dropna(inplace=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_indices(cv, n_splits, X, y, date_col = None):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (11, 7))\n",
    "    \n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=10, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits))\n",
    "    \n",
    "    if date_col is not None:\n",
    "        tick_locations  = ax.get_xticks()\n",
    "        tick_dates = [\" \"] + date_col.iloc[list(tick_locations[1:-1])].astype(str).tolist() + [\" \"]\n",
    "\n",
    "        tick_locations_str = [str(int(i)) for i in tick_locations]\n",
    "        new_labels = ['\\n\\n'.join(x) for x in zip(list(tick_locations_str), tick_dates) ]\n",
    "        ax.set_xticks(tick_locations)\n",
    "        ax.set_xticklabels(new_labels)\n",
    "    \n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+0.2, -.2])\n",
    "    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n",
    "              ['Testing set', 'Training set'], loc=(1.02, .8))\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_importance(model, model_func, params):\n",
    "    importance_function = model.coef_[0]\n",
    "    if model_func in [ut.ModelFunc.XGBOOST_CLASS, ut.ModelFunc.DECISION_TREE_CLASS, ut.ModelFunc.RANDOM_FOREST_CLASS, \\\n",
    "                      ut.ModelFunc.KNN_CLASS, ]:\n",
    "        importance_function = model.feature_importances_\n",
    "    if model_func in [ut.ModelFunc.CATBOOST_CLASS, ]:\n",
    "        importance_function = model.get_feature_importance()\n",
    "\n",
    "    ut.top_n_weighted_factors(importance_function, params['features'], params['top'])\n",
    "    return importance_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Blending, Stacking, Ensemble\n",
    "def fit_models(model_funcs, X_train, y_train, X_val=None, y_val=None):\n",
    "    models = list()\n",
    "    for model_func in model_funcs:\n",
    "        params = ut.get_model_params(model_func)\n",
    "        if X_val is None or y_val is None:\n",
    "            model = ut.model_fit(model_func, X_train, y_train, params)\n",
    "        else:\n",
    "            if model_func is ut.ModelFunc.CATBOOST_CLASS:\n",
    "                params = dict(params, early_stopping_rounds=50)\n",
    "                model = ut.model_fit_with_eval(model_func, X_train, y_train, eval_set=(X_val, y_val), params=params)\n",
    "            else:\n",
    "                model = ut.model_fit(model_func, X_train, y_train, params)\n",
    "        models.append(model)\n",
    "\n",
    "    return models\n",
    "\n",
    "def predict_models(models, X_train, X_val, X_test):\n",
    "    models_proba = list()\n",
    "    for model in models:\n",
    "        models_proba.append({\n",
    "            'train': np.array(model.predict_proba(X_train)[:, 1]),\n",
    "            'val': np.array(model.predict_proba(X_val)[:, 1]),\n",
    "            'test': np.array(model.predict_proba(X_test)[:, 1])\n",
    "        })\n",
    "\n",
    "    return models_proba\n",
    "\n",
    "\n",
    "def blending_pred(*args):\n",
    "    return sum(args) / len(args)\n",
    "    \n",
    "def stacking_pred(*args):\n",
    "    return np.column_stack(args)\n",
    "\n",
    "# stacking_pred(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'emaf': 20,\n",
    "    'emam': 100,\n",
    "    'emas': 150,\n",
    "    'rsi': 14,\n",
    "    'macd': [12, 26, 9],\n",
    "    'max_train_size': 180,\n",
    "    'test_size': 60,\n",
    "    # 'max_train_size': 90,\n",
    "    # 'test_size': 30\n",
    "}\n",
    "\n",
    "trend_indicators = [ 'emaf', 'emam', 'emas', 'rsi', 'macd', 'adx']\n",
    "lag_periods = 7 #3 # depends on timeframe, 7 days - ???\n",
    "min_outliers=.23\n",
    "max_outliers=.77\n",
    "threshold = 0.6 # ???\n",
    "use_stacking = True\n",
    "use_blending = False\n",
    "\n",
    "fe = FeatureEngineering(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ensemble(model_funcs, data_with_features, features):\n",
    "    for train_data, val_data, test_data in tqdm(fe.split_data(data_with_features)):\n",
    "        X_train, y_train = train_data[features], train_data['Target']\n",
    "        X_val, y_val     = val_data[features], val_data['Target']\n",
    "        X_test, y_test   = test_data[features], test_data['Target']\n",
    "\n",
    "        ## Data normalization\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled = ut.normalize_MinMaxScaler(X_train, X_val, X_test)\n",
    "        # X_train_scaled, X_val_scaled, X_test_scaled = ut.normalize_StandardScaler(X_train, X_val, X_test)\n",
    "    \n",
    "        ## Modeling\n",
    "        # models = models_fit(model_funcs, X_train_scaled, y_train, X_val=X_val, y_val=y_val)\n",
    "        models = fit_models(model_funcs, X_train_scaled, y_train)\n",
    "\n",
    "        ## Prediction on train, val and test samples\n",
    "        predict_dict = predict_models(models, X_train_scaled, X_val_scaled, X_test_scaled)\n",
    "\n",
    "        yield ( predict_dict, y_train, y_val, y_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    models = list()\n",
    "    # models.append(ut.ModelFunc.LOGISTIC_REG)\n",
    "    # models.append(ut.ModelFunc.LINEAR_REG)\n",
    "    # models.append(ut.ModelFunc.KNN_REG)\n",
    "    # models.append(ut.ModelFunc.DECISION_TREE_REG)\n",
    "    # models.append(ut.ModelFunc.RANDOM_FOREST_REG)\n",
    "    # models.append(ut.ModelFunc.CATBOOST_REG)\n",
    "    # models.append(ut.ModelFunc.XGBOOST_REG)\n",
    "\n",
    "    # models.append(ut.ModelFunc.XGBOOST_CLASS)\n",
    "    # models.append(ut.ModelFunc.CATBOOST_CLASS)\n",
    "\n",
    "    # models.append(ut.ModelFunc.RANDOM_FOREST_CLASS)\n",
    "    models.append(ut.ModelFunc.DECISION_TREE_CLASS)\n",
    "    models.append(ut.ModelFunc.KNN_CLASS)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== symbol: BTC-USD, stacking: True, blending: False ===\n",
      "Outliers detected: 0\n",
      "Train size: 1973, Val size: 31, Test size: 31\n",
      "Models: [<function decision_tree_classifier_model at 0x7fe792c99f80>, <function knn_classifier_model at 0x7fe792c9a0c0>]\n",
      "=== Train sample metrics ===\n",
      "ROC AUC: 0.6078\n",
      "   Cutoff   Precision     Recall   Accuracy   F1-Score\n",
      "0    50.0   56.161864  87.992315  57.425241  68.562874\n",
      "1    60.0   96.629213   8.261287  51.444501  15.221239\n",
      "2    70.0  100.000000   3.842459  49.265079   7.400555\n",
      "3    80.0    0.000000   0.000000  47.237709   0.000000\n",
      "=== Val sample metrics ===\n",
      "ROC AUC: 0.5192\n",
      "   Cutoff  Precision    Recall   Accuracy  F1-Score\n",
      "0    50.0      100.0  3.846154  19.354839  7.407407\n",
      "1    60.0      100.0  3.846154  19.354839  7.407407\n",
      "2    70.0      100.0  3.846154  19.354839  7.407407\n",
      "3    80.0        0.0  0.000000  16.129032  0.000000\n",
      "=== Test sample metrics ===\n",
      "ROC AUC: 0.4979\n",
      "   Cutoff  Precision  Recall   Accuracy   F1-Score\n",
      "0    50.0       50.0    6.25  48.387097  11.111111\n",
      "1    60.0       50.0    6.25  48.387097  11.111111\n",
      "2    70.0       50.0    6.25  48.387097  11.111111\n",
      "3    80.0        0.0    0.00  48.387097   0.000000\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "symbols =['BTC-USD']\n",
    "for symbol in symbols:\n",
    "# for name in tqdm(tickers):\n",
    "    print(f'=== symbol: {symbol}, stacking: {use_stacking}, blending: {use_blending} ===')\n",
    "\n",
    "    data = get_data(crypto_dir, symbol)\n",
    "    data.drop(columns=['chg', 'vol_chg'], inplace=True) # Could it be as features ?\n",
    "\n",
    "    df = fe.clear_invalid_targets(fe.add_target(fe.enrich_with_indicators(data), lag_periods))\n",
    "    # df = fe.clear_invalid_targets(fe.add_target2(fe.enrich_with_indicators(data)))\n",
    "    df = fe.validate_outliers(df, 'Close', min_outliers, max_outliers)\n",
    "    # # print(df.isnull().sum())\n",
    "    \n",
    "    ## Store data\n",
    "    # df = merge_and_store_data(df, symbol, compress=True) # Store data\n",
    "    # print(df.isnull().sum())\n",
    "\n",
    "    ## Add features\n",
    "    OHLCV = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "    # Trend features\n",
    "    data_with_trend, new_trend_features = fe.create_trend_features(df, OHLCV, lag_periods)\n",
    "    # print(data_with_trend.isnull().sum())\n",
    "    features = new_trend_features + trend_indicators  + ['Target']\n",
    "    data_with_features = data_with_trend[features + ['Date']]\n",
    "\n",
    "    # Rolling features\n",
    "    # window_sizes = [7, 14, 30]\n",
    "    # data_with_rolling, new_rolling_features = fe.create_rolling_features(df, OHLCV, window_sizes)\n",
    "    # features = new_rolling_features + trend_indicators + ['Target']\n",
    "    # data_with_features = data_with_rolling[features + ['Date']]\n",
    "\n",
    "    # print(len(data_with_features))\n",
    "    # print(data_with_features.isnull().sum())\n",
    "    data_with_features.set_index('Date', inplace=True)\n",
    "    # display(data_with_features.tail(10))\n",
    "\n",
    "    test_start_date = pd.to_datetime(data_with_features.index.max()) - pd.DateOffset(months=1)\n",
    "    val_start_date = pd.to_datetime(data_with_features.index.max()) - pd.DateOffset(months=2)\n",
    "\n",
    "    train_data = data_with_features[pd.to_datetime(data_with_features.index) < val_start_date]  # все, что до предпоследнего месяца\n",
    "    val_data = data_with_features[(pd.to_datetime(data_with_features.index) >= val_start_date) & (pd.to_datetime(data_with_features.index) < test_start_date)]  # предпоследний месяц\n",
    "    test_data = data_with_features[pd.to_datetime(data_with_features.index) >= test_start_date]  # последний месяц\n",
    "    # print(data_with_features.index[-1], train_data.index[-1], val_data.index[-1], test_data.index[-1])\n",
    "\n",
    "\n",
    "    X_train = train_data[new_trend_features]\n",
    "    y_train = train_data['Target']\n",
    "\n",
    "    X_val = val_data[new_trend_features]\n",
    "    y_val = val_data['Target']\n",
    "\n",
    "    X_test = test_data[new_trend_features]\n",
    "    y_test = test_data['Target']\n",
    "\n",
    "    # Проверим размерности\n",
    "    print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}, Test size: {len(X_test)}\")\n",
    "\n",
    "    model_funcs = get_models()\n",
    "    print(f'Models: {model_funcs}')\n",
    "\n",
    "    ## Data normalization\n",
    "    X_train_scaled, X_val_scaled, X_test_scaled = ut.normalize_MinMaxScaler(X_train, X_val, X_test)\n",
    "    # X_train_scaled, X_val_scaled, X_test_scaled = ut.normalize_StandardScaler(X_train, X_val, X_test)\n",
    "\n",
    "    ## Modeling\n",
    "    # models = models_fit(model_funcs, X_train_scaled, y_train, X_val=X_val, y_val=y_val)\n",
    "    models = fit_models(model_funcs, X_train_scaled, y_train)\n",
    "\n",
    "    ## Prediction on train, val and test samples\n",
    "    predict_dict = predict_models(models, X_train_scaled, X_val_scaled, X_test_scaled)\n",
    "\n",
    "    stacked_train_X = stacking_pred([d['train'] for d in predict_dict][0])\n",
    "    stacked_val_X = stacking_pred([d['val'] for d in predict_dict][0])\n",
    "    stacked_test_X = stacking_pred([d['test'] for d in predict_dict][0])\n",
    "    \n",
    "    final_model_func = ut.ModelFunc.LOGISTIC_REG \n",
    "    final_model = fit_models([final_model_func], stacked_train_X, y_train)[0]\n",
    "    predict_dict = predict_models([final_model], stacked_train_X, stacked_val_X, stacked_test_X)\n",
    "\n",
    "    ensemble_train = [d['train'] for d in predict_dict][0]\n",
    "    ensemble_val = [d['val'] for d in predict_dict][0]\n",
    "    ensemble_test = [d['test'] for d in predict_dict][0]\n",
    "\n",
    "    # ## Display metrics, ROC AUC for train, val and test samples\n",
    "    train_roc_auc = ut.roc_auc_score_metric(y_train, ensemble_train)\n",
    "    val_roc_auc = ut.roc_auc_score_metric(y_val, ensemble_val)\n",
    "    test_roc_auc = ut.roc_auc_score_metric(y_test, ensemble_test)\n",
    "  \n",
    "    print('=== Train sample metrics ===')\n",
    "    print(f'ROC AUC: {train_roc_auc:.4f}')\n",
    "    print(ut.calculate_metrics_table(y_train, ensemble_train))\n",
    "\n",
    "    print('=== Val sample metrics ===')\n",
    "    print(f'ROC AUC: {val_roc_auc:.4f}')\n",
    "    print(ut.calculate_metrics_table(y_val, ensemble_val))\n",
    "\n",
    "    print('=== Test sample metrics ===')\n",
    "    print(f'ROC AUC: {test_roc_auc:.4f}')\n",
    "    print(ut.calculate_metrics_table(y_test, ensemble_test))\n",
    "    print('===========================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== symbol: BTC-USD, stacking: True, blending: False ===\n",
      "Outliers detected: 1\n",
      "Dropping 1 upper outliers\n",
      "Dropping 0 lower outliers\n",
      "Models: [<function decision_tree_classifier_model at 0x7fe792c99f80>, <function knn_classifier_model at 0x7fe792c9a0c0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:00, 40.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2160, 2027]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m     final_model_func \u001b[38;5;241m=\u001b[39m ut\u001b[38;5;241m.\u001b[39mModelFunc\u001b[38;5;241m.\u001b[39mLOGISTIC_REG \n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# final_model = fit_models([final_model_func], stacked_train_X, y_train_total)[0]\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     final_model \u001b[38;5;241m=\u001b[39m fit_models([final_model_func], stacked_train_X, data_with_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     76\u001b[0m     predict_dict \u001b[38;5;241m=\u001b[39m predict_models([final_model], stacked_train_X, stacked_val_X, stacked_test_X)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m use_blending:\n",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m, in \u001b[0;36mfit_models\u001b[0;34m(model_funcs, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m      5\u001b[0m params \u001b[38;5;241m=\u001b[39m ut\u001b[38;5;241m.\u001b[39mget_model_params(model_func)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m y_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m ut\u001b[38;5;241m.\u001b[39mmodel_fit(model_func, X_train, y_train, params)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_func \u001b[38;5;129;01mis\u001b[39;00m ut\u001b[38;5;241m.\u001b[39mModelFunc\u001b[38;5;241m.\u001b[39mCATBOOST_CLASS:\n",
      "File \u001b[0;32m~/workspaces/ML/wrk/hw1/utils.py:163\u001b[0m, in \u001b[0;36mmodel_fit\u001b[0;34m(model_func, X_train, y_train, params)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_fit\u001b[39m(model_func, X_train, y_train, params):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_func(X_train, y_train, params)\n",
      "File \u001b[0;32m~/workspaces/ML/wrk/hw1/utils.py:68\u001b[0m, in \u001b[0;36mlogistic_regression_model\u001b[0;34m(X_train, y_train, params)\u001b[0m\n\u001b[1;32m     66\u001b[0m model_params \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     67\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression()\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_params)\n\u001b[0;32m---> 68\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/workspaces/ML/wrk/hw1/otus/lib64/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/workspaces/ML/wrk/hw1/otus/lib64/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1223\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1224\u001b[0m     X,\n\u001b[1;32m   1225\u001b[0m     y,\n\u001b[1;32m   1226\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1227\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_dtype,\n\u001b[1;32m   1228\u001b[0m     order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1229\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1230\u001b[0m )\n\u001b[1;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m~/workspaces/ML/wrk/hw1/otus/lib64/python3.11/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/workspaces/ML/wrk/hw1/otus/lib64/python3.11/site-packages/sklearn/utils/validation.py:1320\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1302\u001b[0m     X,\n\u001b[1;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1316\u001b[0m )\n\u001b[1;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m-> 1320\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/workspaces/ML/wrk/hw1/otus/lib64/python3.11/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2160, 2027]"
     ]
    }
   ],
   "source": [
    "symbols =['BTC-USD']\n",
    "for symbol in symbols:\n",
    "# for name in tqdm(tickers):\n",
    "    print(f'=== symbol: {symbol}, stacking: {use_stacking}, blending: {use_blending} ===')\n",
    "\n",
    "    data = get_data(crypto_dir, symbol)\n",
    "    data.drop(columns=['chg', 'vol_chg'], inplace=True) # Could it be as features ?\n",
    "\n",
    "    df = fe.clear_invalid_targets(fe.add_target(fe.enrich_with_indicators(data), lag_periods))\n",
    "    # df = fe.clear_invalid_targets(fe.add_target2(fe.enrich_with_indicators(data)))\n",
    "    df = fe.validate_outliers(df, 'Close', min_outliers, max_outliers)\n",
    "    # # print(df.isnull().sum())\n",
    "    \n",
    "    ## Store data\n",
    "    # df = merge_and_store_data(df, symbol, compress=True) # Store data\n",
    "    # print(df.isnull().sum())\n",
    "\n",
    "    ## Add features\n",
    "    OHLCV = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "    # Trend features\n",
    "    data_with_trend, new_trend_features = fe.create_trend_features(df, OHLCV, lag_periods)\n",
    "    # print(data_with_trend.isnull().sum())\n",
    "    features = new_trend_features + trend_indicators  + ['Target']\n",
    "    data_with_features = data_with_trend[features + ['Date']]\n",
    "\n",
    "    # Rolling features\n",
    "    # window_sizes = [7, 14, 30]\n",
    "    # data_with_rolling, new_rolling_features = fe.create_rolling_features(df, OHLCV, window_sizes)\n",
    "    # features = new_rolling_features + trend_indicators + ['Target']\n",
    "    # data_with_features = data_with_rolling[features + ['Date']]\n",
    "\n",
    "    # print(len(data_with_features))\n",
    "    # print(data_with_features.isnull().sum())\n",
    "    data_with_features.set_index('Date', inplace=True)\n",
    "    # display(data_with_features.tail(10))\n",
    "\n",
    "\n",
    "    model_funcs = get_models()\n",
    "    print(f'Models: {model_funcs}')\n",
    "\n",
    "    ## Split, predict\n",
    "    y_train_pred_prob = list()\n",
    "    y_val_pred_prob = list()\n",
    "    y_test_pred_prob = list()\n",
    "    y_train_total = pd.DataFrame()\n",
    "    y_val_total = pd.DataFrame()\n",
    "    y_test_total = pd.DataFrame()\n",
    "\n",
    "    for predict_dict, y_train, y_val, y_test in predict_ensemble(model_funcs, data_with_features, features):\n",
    "        y_train_total = pd.concat([y_train_total, y_train], ignore_index=True)\n",
    "        y_val_total = pd.concat([y_val_total, y_val], ignore_index=True)\n",
    "        y_test_total = pd.concat([y_test_total, y_test], ignore_index=True)\n",
    "\n",
    "        y_train_pred_prob.append([d['train'] for d in predict_dict][0])\n",
    "        y_val_pred_prob.append([d['val'] for d in predict_dict][0])\n",
    "        y_test_pred_prob.append([d['test'] for d in predict_dict][0])\n",
    "\n",
    "    ## 2D-array\n",
    "    train_pred_prob = hstack(y_train_pred_prob)\n",
    "    val_pred_prob = hstack(y_val_pred_prob)\n",
    "    test_pred_prob = hstack(y_test_pred_prob)\n",
    "\n",
    "    print(len(train_pred_prob))\n",
    "\n",
    "\n",
    "    ## Final model using whole data\n",
    "    if use_stacking:\n",
    "        stacked_train_X = stacking_pred(train_pred_prob).reshape(-1,1)\n",
    "        stacked_val_X = stacking_pred(val_pred_prob).reshape(-1,1)\n",
    "        stacked_test_X = stacking_pred(test_pred_prob).reshape(-1,1)\n",
    "        \n",
    "        final_model_func = ut.ModelFunc.LOGISTIC_REG \n",
    "        # final_model = fit_models([final_model_func], stacked_train_X, y_train_total)[0]\n",
    "        final_model = fit_models([final_model_func], stacked_train_X, data_with_features['Target'])[0]\n",
    "        predict_dict = predict_models([final_model], stacked_train_X, stacked_val_X, stacked_test_X)\n",
    "\n",
    "    elif use_blending:\n",
    "        blended_train_X = stacking_pred(train_pred_prob).reshape(-1,1)\n",
    "        blended_val_X = stacking_pred(val_pred_prob).reshape(-1,1)\n",
    "        blended_test_X = stacking_pred(test_pred_prob).reshape(-1,1)\n",
    "\n",
    "        final_model_func = ut.ModelFunc.LOGISTIC_REG\n",
    "        final_model = fit_models([final_model_func], blended_train_X, y_train_total)[0]\n",
    "        predict_dict = predict_models([final_model], blended_train_X, blended_val_X, blended_test_X)\n",
    "\n",
    "    else:\n",
    "        final_model_func = model_funcs[0]\n",
    "        final_model = fit_models([final_model_func], train_pred_prob.reshape(-1,1), y_train_total)[0]\n",
    "        predict_dict = predict_models([final_model], train_pred_prob.reshape(-1,1), val_pred_prob.reshape(-1,1),\\\n",
    "                                       test_pred_prob.reshape(-1,1))\n",
    "\n",
    "        # params = {\n",
    "        #     'features': features,\n",
    "        #     'top': 5,\n",
    "        # }\n",
    "        # importance_features = show_importance(final_model, final_model_func, params)\n",
    "\n",
    "    ensemble_train = [d['train'] for d in predict_dict][0]\n",
    "    ensemble_val = [d['val'] for d in predict_dict][0]\n",
    "    ensemble_test = [d['test'] for d in predict_dict][0]\n",
    "\n",
    "    # ## Display metrics, ROC AUC for train, val and test samples\n",
    "    train_roc_auc = ut.roc_auc_score_metric(y_train_total, ensemble_train)\n",
    "    val_roc_auc = ut.roc_auc_score_metric(y_val_total, ensemble_val)\n",
    "    test_roc_auc = ut.roc_auc_score_metric(y_test_total, ensemble_test)\n",
    "  \n",
    "    print('=== Train sample metrics ===')\n",
    "    print(f'ROC AUC: {train_roc_auc:.4f}')\n",
    "    print(ut.calculate_metrics_table(y_train_total, ensemble_train))\n",
    "\n",
    "    print('=== Val sample metrics ===')\n",
    "    print(f'ROC AUC: {val_roc_auc:.4f}')\n",
    "    print(ut.calculate_metrics_table(y_val_total, ensemble_val))\n",
    "\n",
    "    print('=== Test sample metrics ===')\n",
    "    print(f'ROC AUC: {test_roc_auc:.4f}')\n",
    "    print(ut.calculate_metrics_table(y_test_total, ensemble_test))\n",
    "    print('===========================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
